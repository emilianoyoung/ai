---
category: news
title: "NLP Year in Review — 2019"
excerpt: "Researchers from Facebook AI also recently published a method based on an all-attention layer for improving the efficiency of a Transformer language model. More work from this research group ..."
publishedDateTime: 2020-01-02T13:10:00Z
sourceUrl: https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19
type: article
quality: 54
heat: 54
published: false

provider:
  name: Medium
  domain: medium.com
  images:
    - url: /assets/images/organizations/medium.com-50x50.jpg
      width: 50
      height: 50

topics:
  - AI
  - Facebook AI

images:
  - url: https://miro.medium.com/max/1200/1*T08rCNctBW5zUvol0gfIig.png
    width: 1200
    height: 764
    title: "NLP Year in Review — 2019"

secured: "oMS/OTlrHaEKKkfvrcyNu6PG5/eRq5nOd6DuW8zmHZPDN+0gSeGzea9Fx7V0QXoslQN5ClX1+XRwF9aKJjmU5JJOhzkkfTpNr/IpmsOGZm6k4OKTz5ur6Ri/IEQ9SjpsypCJe8uFntMRTjk543kJK4joeZLx3ZnE2rXYDtZNXYAy0C1+A66pDxfSbRQSCffqo0ZpavUUOyo32MalnAkqXOQeTARh1ytvqyyKITRGHjVKFaYebTWpjrKDMkJ+3HqM9/4pRrNYQ3DWFVV2m4oxkA4RW3ut5uvx7K82zrtmmBAF4euMiSLkUsoTYmYRHsGn;YnTbYOilHArIkBexYtA1Lg=="
---

