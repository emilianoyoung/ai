---
category: news
title: "NLP Year in Review — 2019"
excerpt: "Researchers from Facebook AI also recently published a method based on an all-attention layer for improving the efficiency of a Transformer language model. More work from this research group ..."
publishedDateTime: 2020-01-02T13:10:00Z
sourceUrl: https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19
type: article
quality: 54
heat: 54
published: false

provider:
  name: Medium
  domain: medium.com
  images:
    - url: /assets/images/organizations/medium.com-50x50.jpg
      width: 50
      height: 50

topics:
  - AI
  - Facebook AI

images:
  - url: https://miro.medium.com/max/1200/1*T08rCNctBW5zUvol0gfIig.png
    width: 1200
    height: 764
    title: "NLP Year in Review — 2019"

secured: "Dy3MCse2fuiYs+7rMterERBjXLIUMdIYRUH57yV6jR0dvzSqJOeQEH8XroXTHXvR4MuuCnvUl3xy39UO3EH1oPoTxcy9Shk4lfLiGggeRWj24uD1tGop7ex5SnWGjxgOUSX9oNXqFonpl56rZDJtArmgNKDfdJ0XAKaKMECIc/ttdCL0r4Dwcdz7srbDqIZzKaUOvxfUbhhLFeF4Uwmjl8A9fIOyDVRbNp6lh1p4iE+jF1WNBXX7zWYF4XG7TYklGt1v4Pzsgto3xBIk64lVi8BiL8XgLN8rRuc+8tp/cm7fhD1qOAM0s6zK6pz497jp;TvkNXS9ARy1eMd59T87U8A=="
---

